{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "135fbe27-1b98-489a-a2ca-63661e801b0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact the validity of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad46523-d263-4022-941b-603e9955bbbb",
   "metadata": {},
   "source": [
    "ANOVA (Analysis of Variance) is a statistical test used to compare means of three or more groups simultaneously. It is an extension of the t-test, which is used for comparing means of two groups. ANOVA is based on several assumptions, and violating these assumptions can impact the validity of the results.\n",
    "\n",
    "Assumptions for using ANOVA:\n",
    "\n",
    "1. Independence: The observations in each group are assumed to be independent of each other. This means that the data points within each group are not influenced by or related to the data points in other groups.\n",
    "\n",
    "2. Normality: The data in each group are assumed to follow a normal distribution. ANOVA is relatively robust to moderate departures from normality, especially with larger sample sizes. However, severe departures from normality can affect the validity of the results.\n",
    "\n",
    "3. Homogeneity of variance (Homoscedasticity): The variances of the data in each group are assumed to be equal. This means that the spread of data points around the group means should be similar for all groups.\n",
    "\n",
    "4. Random sampling: The data should be obtained from random sampling from the respective populations.\n",
    "\n",
    "Examples of violations and their impact on ANOVA results:\n",
    "\n",
    "1. Non-independence: If the observations in one group are somehow related or dependent on the observations in another group, the independence assumption is violated. This can lead to increased Type 1 errors (false positives) and biased results. For example, in a study comparing the effects of different teaching methods on student performance, if the same students are assigned to multiple teaching methods, the independence assumption would be violated.\n",
    "\n",
    "2. Non-normality: When the data in one or more groups deviate significantly from a normal distribution, the validity of ANOVA results can be compromised. Skewed or heavy-tailed distributions may lead to inaccurate p-values and incorrect conclusions. This can happen when sample sizes are small or if outliers are present in the data.\n",
    "\n",
    "3. Heteroscedasticity: Violation of the assumption of equal variances across groups can lead to imprecise estimates of the group means and inflated Type 1 error rates. In other words, if the variances differ significantly between groups, the ANOVA results may be less reliable. This is especially critical when sample sizes are unequal.\n",
    "\n",
    "4. Non-random sampling: If the data is collected in a non-random or biased manner, the generalizability of the ANOVA results may be limited. Biased sampling can lead to inaccurate population inferences.\n",
    "\n",
    "When these assumptions are not met, researchers may need to consider alternative statistical tests or apply data transformations to address the issues. Non-parametric tests, such as the Kruskal-Wallis test, can be used when the normality or equal variance assumptions are violated. Additionally, bootstrapping methods and robust statistical techniques can be helpful in situations with non-normal data or heteroscedasticity. Careful consideration of the data and its characteristics is essential to ensure the validity and reliability of ANOVA results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b62dc-d4ed-4ed4-ab68-533eb6d98b20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q2. What are the three types of ANOVA, and in what situations would each be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ec6156-3e69-408a-a01e-6dfd224a359b",
   "metadata": {},
   "source": [
    "ANOVA (Analysis of Variance) can be categorized into three main types based on the number of factors or independent variables involved:\n",
    "\n",
    "1. One-Way ANOVA:\n",
    "One-Way ANOVA is used when there is only one factor (independent variable) with three or more levels or groups. It is used to compare means among multiple groups and determine if there are significant differences between the group means. One-Way ANOVA is appropriate when you want to assess the effect of a single categorical variable on a continuous dependent variable.\n",
    "\n",
    "Example situations for One-Way ANOVA:\n",
    "- A study comparing the mean test scores of students from different schools (e.g., public, private, charter).\n",
    "- Evaluating the effectiveness of three different teaching methods on student performance (e.g., traditional lecture, active learning, online modules).\n",
    "\n",
    "2. Two-Way ANOVA:\n",
    "Two-Way ANOVA is used when there are two factors (independent variables) with two or more levels each. It allows you to simultaneously examine the main effects of each factor and their interaction effect on the dependent variable. Two-Way ANOVA is appropriate when you want to study the combined effects of two categorical variables on a continuous dependent variable.\n",
    "\n",
    "Example situations for Two-Way ANOVA:\n",
    "- A drug trial comparing the effects of different drug dosages (Factor 1) and gender (Factor 2) on blood pressure.\n",
    "- Studying the influence of two factors, such as temperature and humidity, on plant growth.\n",
    "\n",
    "3. N-Way ANOVA (N-Way refers to three or more factors):\n",
    "N-Way ANOVA extends the concept of Two-Way ANOVA to include three or more factors. It allows you to study the interactions and main effects of multiple independent variables on a continuous dependent variable simultaneously.\n",
    "\n",
    "Example situations for N-Way ANOVA:\n",
    "- Investigating the effects of temperature, pH level, and different fertilizer types on crop yield.\n",
    "- Analyzing the impact of education level, income level, and region on consumer spending behavior.\n",
    "\n",
    "It's essential to choose the appropriate type of ANOVA based on the research design and the number of factors under investigation. The choice of ANOVA depends on the specific research question and the experimental or observational setup. Each type of ANOVA provides valuable insights into the relationships between variables and helps researchers draw meaningful conclusions from their data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7605da1a-9711-4161-b828-107a782f529e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6792a68e-ea00-4d80-a137-9c3f80617eba",
   "metadata": {},
   "source": [
    "Partitioning of variance in ANOVA refers to the process of decomposing the total variability in the data into different sources of variation. ANOVA achieves this decomposition by dividing the total sum of squares (SS) into several components, each representing a specific source of variability. Understanding this concept is crucial as it allows researchers to gain insights into the contributions of different factors to the overall variability in the data, facilitating the interpretation of results and helping draw meaningful conclusions from the analysis.\n",
    "\n",
    "The partitioning of variance in ANOVA is typically done using the following components:\n",
    "\n",
    "1. Total Sum of Squares (SST): SST represents the total variability in the data and is the sum of the squared differences between each data point and the overall mean. It measures how much the data points vary from the overall mean.\n",
    "\n",
    "2. Between-Group Sum of Squares (SSB): SSB represents the variability between the group means. It is the sum of the squared differences between each group mean and the overall mean. SSB measures how much the group means differ from each other.\n",
    "\n",
    "3. Within-Group Sum of Squares (SSW): SSW represents the variability within each group. It is the sum of the squared differences between each data point and its respective group mean. SSW measures how much the data points within each group deviate from their group mean.\n",
    "\n",
    "The relationship among these components can be expressed as:\n",
    "\n",
    "\\[ SST = SSB + SSW \\]\n",
    "\n",
    "The F-ratio, which is calculated as the ratio of the variance between groups to the variance within groups, is used to test the hypothesis of whether the group means are equal or not. If there are significant differences between the group means, the F-ratio will be large, leading to rejection of the null hypothesis (i.e., the means are equal).\n",
    "\n",
    "The importance of understanding the partitioning of variance in ANOVA includes:\n",
    "\n",
    "1. Identifying sources of variation: By partitioning the variance, ANOVA allows researchers to identify which factors contribute significantly to the variability in the data. This helps in understanding the impact of different factors on the dependent variable.\n",
    "\n",
    "2. Assessing significance: ANOVA allows researchers to test the statistical significance of the differences between group means. By comparing the between-group variability to the within-group variability, researchers can determine whether the observed differences are likely due to chance or are statistically significant.\n",
    "\n",
    "3. Informing further analysis: Understanding the partitioning of variance can guide researchers in conducting post-hoc tests or follow-up analyses to explore specific group differences.\n",
    "\n",
    "4. Comparing multiple groups simultaneously: ANOVA is particularly useful when comparing means among three or more groups, as it avoids the problem of conducting multiple pairwise comparisons, which can lead to an increased risk of Type 1 errors (false positives).\n",
    "\n",
    "In conclusion, partitioning of variance in ANOVA provides valuable insights into the data, helps in making informed decisions, and allows researchers to draw meaningful conclusions about the relationships between variables under study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140c695b-1070-498b-88ce-d048905ccd30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4370965b-be23-4997-8dc4-e62ea6558428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST:  141.6\n",
      "SSE:  3.4000000000000012\n",
      "SSR:  138.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stat\n",
    "\n",
    "group1 = [10,12,14,16,18]\n",
    "group2 = [11,13,15,17,19]\n",
    "group3 = [10,13,14,17,20]\n",
    "\n",
    "all_data = np.concatenate((group1,group2,group3))\n",
    "overall_mean = np.mean(all_data)\n",
    "\n",
    "# calculate sst SST = Σ(yᵢ - ȳ)²\n",
    "SST = np.sum((all_data - overall_mean) ** 2)\n",
    "\n",
    "g1_mean = np.mean(group1)\n",
    "g2_mean = np.mean(group2)\n",
    "g3_mean = np.mean(group3)\n",
    "\n",
    "# calculate SSE SSE = Σ(nᵢ * (ȳᵢ - ȳ)²)\n",
    "SSE = np.sum((g1_mean - overall_mean) ** 2) * len(group1) + \\\n",
    "      np.sum((g2_mean - overall_mean) **2) * len(group2) + \\\n",
    "      np.sum((g2_mean - overall_mean) **2) * len(group3)\n",
    "\n",
    "# calculate SSR\n",
    "\n",
    "SSR = SST - SSE\n",
    "\n",
    "print(\"SST: \" , SST)\n",
    "print(\"SSE: \" , SSE)\n",
    "print(\"SSR: \", SSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6661fb1-7361-4513-9a4e-42f78b22a270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Way-Anova:  F_onewayResult(statistic=0.12103746397694526, pvalue=0.887068742991468)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "group1 = [10,12,14,16,18]\n",
    "group2 = [11,13,15,17,19]\n",
    "group3 = [10,13,14,17,20]\n",
    "\n",
    "one_way_ANOVA = stats.f_oneway(group1,group2,group3)\n",
    "print(\"One-Way-Anova: \" ,one_way_ANOVA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ce07ea-9e06-44f9-bdc5-7102e603f040",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac08bd33-4f17-42cc-a68b-9f3dfbfd8e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   sum_sq   df          F    PR(>F)\n",
      "factor_a            108.0  1.0  29.793103  0.000603\n",
      "factor_b            338.0  1.0  93.241379  0.000011\n",
      "factor_a:factor_b     8.0  1.0   2.206897  0.175699\n",
      "Residual             29.0  8.0        NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "data = np.array([[10, 15, 20],\n",
    "                 [12, 18, 24],\n",
    "                 [14, 21, 28],\n",
    "                 [16, 24, 32]])\n",
    "\n",
    "flattened_data = data.flatten()\n",
    "factor_a = np.repeat([1, 2], 6)  \n",
    "factor_b = np.tile([1, 2, 3], 4) \n",
    "df = pd.DataFrame({'data': flattened_data, 'factor_a': factor_a, 'factor_b': factor_b})\n",
    "\n",
    "formula = 'data ~ factor_a + factor_b + factor_a:factor_b'\n",
    "model = ols(formula, data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(anova_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "684ce210-3e3d-4961-8f6b-32d592830f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Effect of A: 2.000000000000007\n",
      "Main Effect of B: 3.5000000000000018\n",
      "Interaction Effect: 1.999999999999993\n"
     ]
    }
   ],
   "source": [
    "main_effect_a = model.params['factor_a']\n",
    "main_effect_b = model.params['factor_b']\n",
    "interaction_effect = model.params['factor_a:factor_b']\n",
    "\n",
    "print(\"Main Effect of A:\", main_effect_a)\n",
    "print(\"Main Effect of B:\", main_effect_b)\n",
    "print(\"Interaction Effect:\", interaction_effect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bb0598-fed9-4c1c-9498-2303fc616605",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02. What can you conclude about the differences between the groups, and how would you interpret these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcad0d5-aa55-42c0-8c08-0a2d722c0465",
   "metadata": {},
   "source": [
    "In a one-way ANOVA, the F-statistic is used to test whether there are statistically significant differences in means among three or more groups. The p-value associated with the F-statistic indicates the probability of observing such differences by random chance alone. A small p-value suggests that the differences between the groups are unlikely to be solely due to random variation.\n",
    "\n",
    "In your case, you obtained an F-statistic of 5.23 and a p-value of 0.02. Here's how you can interpret these results:\n",
    "\n",
    "1. **F-Statistic (5.23):** The F-statistic represents the ratio of the variation between group means to the variation within groups. A larger F-statistic indicates a larger difference between group means compared to the variability within each group. In your case, the calculated F-statistic suggests that there are differences between the group means.\n",
    "\n",
    "2. **P-Value (0.02):** The p-value indicates the probability of observing the observed differences (or more extreme differences) between group means if the null hypothesis is true. The null hypothesis states that there are no significant differences between the groups. A p-value of 0.02 means that if the null hypothesis is true, there is a 2% chance of observing the obtained differences between groups due to random chance.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Since the p-value (0.02) is less than a typical significance level (such as 0.05), you would reject the null hypothesis. This suggests that there are statistically significant differences between at least one pair of groups. In other words, the data provide enough evidence to conclude that the means of at least two groups are not equal.\n",
    "\n",
    "However, the ANOVA itself does not tell you which specific groups are different from each other. To identify which groups are different, you may need to perform post hoc tests (e.g., Tukey's Honestly Significant Difference test) or examine confidence intervals for group means.\n",
    "\n",
    "Keep in mind that a significant result in ANOVA only indicates the presence of differences; it doesn't provide information about the nature or direction of those differences. Further analyses are often needed to understand the specific group differences and their implications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7018b8e0-232d-478a-8844-22385398c6c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential consequences of using different methods to handle missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364d6f7b-8e33-48e4-b44f-37aa2708b6b0",
   "metadata": {},
   "source": [
    "Handling missing data in repeated measures ANOVA can be challenging because each participant contributes data at multiple time points, and missing data can occur at different time points for different participants. There are several methods to handle missing data in this context, and the choice of method can have consequences on the validity and interpretation of the results.\n",
    "\n",
    "1. Listwise deletion (Complete Case Analysis):\n",
    "In listwise deletion, any participant with missing data at any time point is entirely excluded from the analysis. This method reduces the sample size and can introduce bias if the missing data are not missing completely at random (MCAR). It may lead to less precise estimates and reduced statistical power.\n",
    "\n",
    "2. Pairwise deletion:\n",
    "With pairwise deletion, participants with missing data for specific time points are excluded only for those time points. This method retains more data than listwise deletion, but it can still introduce bias if the missing data are not MCAR. The downside is that it may lead to different sample sizes for different time points, potentially affecting the balance of the repeated measures design.\n",
    "\n",
    "3. Mean imputation:\n",
    "Mean imputation involves replacing missing data with the mean of the available data for that time point. While this method allows retention of all participants and time points, it can introduce artificial relationships and underestimate the true variability in the data. It may also distort the correlation structure among time points.\n",
    "\n",
    "4. Last observation carried forward (LOCF):\n",
    "LOCF imputes missing values with the last observed value for that participant. It can lead to biased estimates, especially if the missing data are non-random or if there is a trend in the data over time. LOCF may not be appropriate if the missing data are due to dropouts or intervention effects.\n",
    "\n",
    "5. Multiple imputation:\n",
    "Multiple imputation is a more sophisticated method that generates multiple plausible imputed datasets, incorporating uncertainty about the missing values. It is based on the assumption that the data are missing at random (MAR) rather than MCAR. Multiple imputation can produce more accurate parameter estimates and standard errors compared to single imputation methods. However, it requires additional computational effort.\n",
    "\n",
    "The potential consequences of using different methods to handle missing data include biased parameter estimates, underestimated standard errors, and inflated Type I error rates. Additionally, the choice of method may affect the power of the statistical test and the interpretation of the results.\n",
    "\n",
    "To handle missing data appropriately, it is essential to understand the mechanism of missingness and the assumptions underlying the chosen imputation method. Multiple imputation is generally considered one of the best approaches when the data are missing at random, but it requires careful consideration and validation of the imputation model. Alternatively, sensitivity analyses can be performed to assess the robustness of the results to different methods of handling missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8706f01-783b-4704-8c11-342e913d3425",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide an example of a situation where a post-hoc test might be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc419d15-a024-45e5-94fc-99740c721785",
   "metadata": {},
   "source": [
    "After performing an ANOVA and finding a statistically significant difference among the group means, post-hoc tests are conducted to determine which specific groups differ significantly from each other. Post-hoc tests help avoid Type I errors that may occur when conducting multiple pairwise comparisons.\n",
    "\n",
    "Here are some common post-hoc tests used after ANOVA, along with situations where each one is appropriate:\n",
    "\n",
    "1. Tukey's Honestly Significant Difference (HSD) test:\n",
    "Tukey's HSD test is a conservative post-hoc test that controls the family-wise error rate (FWER). It is suitable when you have equal sample sizes and want to compare all possible pairs of group means.\n",
    "\n",
    "Example situation: Suppose you conduct a study comparing the effectiveness of three different diets (low-carb, Mediterranean, and low-fat) on weight loss. After performing ANOVA and finding a significant difference in weight loss among the groups, you can use Tukey's HSD test to identify which diets have significantly different effects on weight loss.\n",
    "\n",
    "2. Bonferroni correction:\n",
    "The Bonferroni correction is a more stringent method that controls the overall Type I error rate. It divides the desired significance level (usually 0.05) by the number of comparisons to adjust the p-values. It is appropriate when you perform many pairwise comparisons, but it can be overly conservative and may result in lower power.\n",
    "\n",
    "Example situation: In a medical trial, you want to compare the effectiveness of a new drug to a placebo in treating various symptoms. After conducting ANOVA and finding a significant difference among the groups, you decide to use the Bonferroni correction to compare the drug to the placebo for each symptom individually.\n",
    "\n",
    "3. Dunnett's test:\n",
    "Dunnett's test is useful when you have one control group and want to compare it with multiple treatment groups. It is less conservative than Bonferroni correction and is appropriate when you are mainly interested in comparing each treatment group to the control group.\n",
    "\n",
    "Example situation: In a study evaluating the effects of different exercise regimes, you have one control group (no exercise) and several treatment groups (e.g., aerobic exercise, strength training, yoga). After ANOVA reveals a significant difference in a health-related measure, you can use Dunnett's test to compare each exercise group to the control group.\n",
    "\n",
    "4. Scheffe's method:\n",
    "Scheffe's method is a less conservative post-hoc test that is suitable when sample sizes are unequal and group variances are not homogeneous. It allows for a broader range of comparisons and controls the overall Type I error rate.\n",
    "\n",
    "Example situation: In a social science study comparing the performance of three groups of students (small class, medium class, and large class) on an exam, you find a significant difference among the groups. Since the sample sizes are not equal and the variances may differ, you opt for Scheffe's method to compare the means of all three groups.\n",
    "\n",
    "It's important to choose an appropriate post-hoc test based on the specific research question, the design of the study, and the characteristics of the data. Each post-hoc test has its strengths and weaknesses, and selecting the right one will depend on the context of your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f153ef-492d-4c58-9f7a-ca05510c39a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from 50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python to determine if there are any significant differences between the mean weight loss of the three diets. Report the F-statistic and p-value, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd6bd57e-9146-441f-9b1a-6c6889959a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistics:  18.881818181818193\n",
      "P-value:  0.00019661417657625268\n",
      "Mean weight loss for diat A: 12.8\n",
      "Mean weight loss for diat B: 13.4\n",
      "Mean weight loss for diat C: 4.0\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "diat_a = np.array([10,12,14,16,12])\n",
    "diat_b = np.array([11,13,15,17,11])\n",
    "diat_c = np.array([0,2,4,6,8])\n",
    "\n",
    "all_data = np.concatenate((diat_a,diat_b,diat_c))\n",
    "\n",
    "statistics , p_value = stats.f_oneway(diat_a,diat_b,diat_c)\n",
    "alpha = 0.05\n",
    "\n",
    "print(\"F-statistics: \",statistics)\n",
    "print(\"P-value: \",p_value)\n",
    "\n",
    "print(\"Mean weight loss for diat A:\",np.mean(diat_a))\n",
    "print(\"Mean weight loss for diat B:\",np.mean(diat_b))\n",
    "print(\"Mean weight loss for diat C:\",np.mean(diat_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584833d9-ac55-4fcd-8db3-50b6547072cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q10. A company wants to know if there are any significant differences in the average time it takes to complete a task using three different software programs: Program A, Program B, and Program C. They randomly assign 30 employees to one of the programs and record the time it takes each employee to complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects orinteraction effects between the software programs and employee experience level (novice vs. experienced). Report the F-statistics and p-values, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "794bc461-b4a3-45b1-9596-b8e46a76609f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           sum_sq    df         F    PR(>F)\n",
      "C(software)                  81.0   2.0  4.300437  0.053610\n",
      "C(experience)                 NaN   1.0       NaN       NaN\n",
      "C(software):C(experience)     9.8   2.0  0.520300  0.480517\n",
      "Residual                    160.1  17.0       NaN       NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/statsmodels/base/model.py:1871: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 2, but rank is 1\n",
      "  warnings.warn('covariance of constraints does not have full '\n",
      "/opt/conda/lib/python3.10/site-packages/statsmodels/base/model.py:1871: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 1, but rank is 0\n",
      "  warnings.warn('covariance of constraints does not have full '\n",
      "/opt/conda/lib/python3.10/site-packages/statsmodels/base/model.py:1900: RuntimeWarning: invalid value encountered in divide\n",
      "  F /= J\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "\n",
    "data = {'time': [12, 15, 17, 14, 16, 18, 13, 19, 20, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
    "       'software': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'C', 'C', 'C',\n",
    "                   'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n",
    "       'experience': ['novice', 'novice', 'novice', 'novice', 'novice', 'novice',\n",
    "                      'novice', 'novice', 'novice', 'novice','experienced', 'experienced', 'experienced', 'experienced','experienced',\n",
    "                      'experienced', 'experienced', 'experienced', 'experienced','experienced']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "model = ols('time ~ C(software) + C(experience) + C(software) : C(experience)', data=df).fit()\n",
    "\n",
    "anova_table = anova_lm(model , typ = 2)\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58616930-b603-449c-bc9c-38362ad09f76",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q11. An educational researcher is interested in whether a new teaching method improves student test scores. They randomly assign 100 students to either the control group (traditional teaching method) or the experimental group (new teaching method) and administer a test at the end of the semester. Conduct a two-sample t-test using Python to determine if there are any significant differences in test scores between the two groups. If the results are significant, follow up with a post-hoc test to determine which group(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d31fc39c-3b0c-4649-aeae-8d7545a47b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Statistics:  -0.14770978917519928\n",
      "P-Value:  0.8842138175489671\n",
      "Fail to reject the null hypothesis\n",
      "Post-hoc-test\n",
      "\n",
      "    Multiple Comparison of Means - Tukey HSD, FWER=0.05     \n",
      "============================================================\n",
      " group1    group2    meandiff p-adj   lower    upper  reject\n",
      "------------------------------------------------------------\n",
      "control experimental      1.0 0.8842 -13.2233 15.2233  False\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "control = np.array([75, 80, 85, 90, 95, 100, 105, 110, 115, 120])\n",
    "experimental = np.array([76, 81, 86, 91, 96, 101, 106, 111, 116, 121])\n",
    "\n",
    "# two-sample t-test\n",
    "\n",
    "t_stat , p_value = stats.ttest_ind(control,experimental)\n",
    "\n",
    "print(\"T-Statistics: \",t_stat)\n",
    "print(\"P-Value: \",p_value)\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis\")\n",
    "    \n",
    "    \n",
    "# If the results are significant, you can follow up with a post-hoc test to determine which group(s) differ significantly from each other.\n",
    "#One common post-hoc test is Tukey’s HSD test. \n",
    "#You can use the following code to perform it in Python:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "\n",
    "data = {'score': np.concatenate([control,experimental]),\n",
    "        'group': np.repeat(['control','experimental'],len(control))\n",
    "       }\n",
    "print(\"Post-hoc-test\\n\")\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "tukey = pairwise_tukeyhsd(endog=df['score'] , groups=df['group'] , alpha=0.05)\n",
    "\n",
    "print(tukey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3621e81-4964-4a79-a269-8c9128cd0557",
   "metadata": {},
   "source": [
    "# Q12. A researcher wants to know if there are any significant differences in the average daily sales of three retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store on those days. Conduct a repeated measures ANOVA using Python to determine if there are any significant differences in sales between the three stores. If the results are significant, follow up with a post-hoc test to determine which store(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "726660ce-184c-4838-8230-9ef7d535c625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeated Measures ANOVA:\n",
      "F-statistic: 1.523852525725498\n",
      "P-value: 0.08462571015455761\n",
      "\n",
      "No significant differences found\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "data = {\n",
    "    'store':['A','B','C'] * 30,\n",
    "    'day' : np.tile(range(1,31),3), # np.tile to repeat the array\n",
    "    'sales': np.random.randint(50,200,size=90)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Repeated measures ANOVA\n",
    "\n",
    "formula = 'sales ~ store + C(day)'\n",
    "results = ols(formula,data=df).fit()\n",
    "\n",
    "print(\"Repeated Measures ANOVA:\")\n",
    "print(f\"F-statistic: {results.fvalue}\")\n",
    "print(f\"P-value: {results.f_pvalue}\")\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "if results.f_pvalue < alpha:\n",
    "    \n",
    "    posthoc = pairwise_tukeyhsd(df['sales'] , df['store'] , alpha = 0.05)\n",
    "    \n",
    "    print(\"\\nPost-Hoc Tukey's HSD Test\")\n",
    "    print(posthoc)\n",
    "else:\n",
    "    print(\"\\nNo significant differences found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
